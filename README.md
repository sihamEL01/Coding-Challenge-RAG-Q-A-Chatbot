# ðŸ“š RAG Question Answering System â€“ Quorium Coding Challenge

This project implements a **Retrieval-Augmented Generation (RAG)** system for question answering over documents, developed as part of the **Quorium AI Engineer Trainee Coding Challenge**.

The system ingests documents, indexes them using vector embeddings, retrieves the most relevant chunks for a user query, and generates answers **grounded strictly in the retrieved context**.

---

## ðŸš€ Features

- ðŸ“„ Supports document ingestion from **PDF, TXT, and Markdown**
- âœ‚ï¸ Intelligent text chunking with overlap
- ðŸ§  Semantic search using **ChromaDB** vector store
- ðŸ¤– LLM-based answer generation ("meta-llama/Meta-Llama-3-8B-Instruct")
- âš¡ FastAPI backend with clean REST endpoints
- ðŸ’¬ Simple chat-style frontend (Next.js)
- ðŸ³ Docker-first setup for reproducibility

---

## ðŸ§  Architecture Overview

```

User Question
â”‚
â–¼
[Embed Question]
â”‚
â–¼
[ChromaDB Vector Search]
â”‚
â–¼
[Top-K Relevant Chunks]
â”‚
â–¼
[Context Construction]
â”‚
â–¼
[LLM Answer Generation]

```

---

## ðŸ—ï¸ Tech Stack

### Backend
- FastAPI  
- ChromaDB (Vector Store)  
- SentenceTransformers (`all-MiniLM-L6-v2`)  
- "meta-llama/Meta-Llama-3-8B-Instruct"  
- Python 3.13+

### Frontend
- Next.js  
- React  
- Minimal chat interface

### DevOps
- Docker & Docker Compose  
- Custom `docker.sh` wrapper

---

## ðŸ“¦ Project Structure

```

rag_project/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py          # FastAPI app
â”‚   â”‚   â”œâ”€â”€ ingest.py        # Document ingestion
â”‚   â”‚   â”œâ”€â”€ embeddings.py   # Embedding logic
â”‚   â”‚   â”œâ”€â”€ vector_store.py # ChromaDB logic
â”‚   â”‚   â”œâ”€â”€ llm.py          # LLM interface
â”‚   â”‚   â””â”€â”€ utils.py
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ pages/
â”‚   â”œâ”€â”€ components/
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ docker.sh
â”œâ”€â”€ README.md

````

---

## âš™ï¸ Setup & Installation

### 1ï¸âƒ£ Clone the repository

```bash
git clone https://github.com/your-username/rag-quorium-challenge.git
cd rag-quorium-challenge
````

---


### 2ï¸âƒ£ Build Docker images

```bash
./docker.sh build
```

---

###3ï¸âƒ£ Start the system

```bash
./docker.sh up
```

Services will be available at:

* Backend API: `http://localhost:8000`
* Frontend: `http://localhost:3000`

---

## ðŸ“¥ Document Ingestion

To ingest documents into the vector store:

```bash
./docker.sh ingest
```

This process:

* Parses documents (`.pdf`, `.txt`, `.md`)
* Cleans non-content sections
* Splits text into **300â€“500 token chunks**
* Generates embeddings
* Stores them in **ChromaDB**

---

## â“ Question Answering API

### Endpoint

```http
POST /ask
```

### Request Body

```json
{
  "question": "Who gives Jim the black spot?",
  "top_k": 5
}
```

### Response

```json
{
  "answer": "Blind Pew gives Jim the black spot.",
  "sources": [
    "chunk_12",
    "chunk_15"
  ]
}
```

---

## ðŸ›¡ï¸ Grounded Answering (No Hallucinations)

The language model is explicitly instructed to:

* Use **only the retrieved context**
* **Never guess or hallucinate**
* Return:

> *"I don't know based on the provided context."*

when the answer is not present.

This ensures factual and explainable answers.

---

## ðŸ§ª Known Limitations

* Retrieval quality depends on chunking strategy
* Long narrative documents require careful overlap tuning
* LLM responses depend on external API availability

---

## ðŸ“ˆ Future Improvements

* Hybrid retrieval (BM25 + embeddings)
* Metadata-aware search (chapters, entities)
* Cross-encoder re-ranking
* Streaming responses
* Enhanced frontend UX


---

## ðŸ‘¤ Author

**Siham El Yaagoubi**
AI / Data Engineering Student
Quorium Coding Challenge â€“ 2025

```
